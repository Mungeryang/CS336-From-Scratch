{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Trasnformers with HuggingFace\n",
    "\n",
    "![huggingface](https://transformers.run/assets/title.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                           Version                          Editable project location\n",
      "--------------------------------- -------------------------------- -----------------------------\n",
      "accelerate                        1.8.1\n",
      "addict                            2.4.0\n",
      "aiofiles                          25.1.0\n",
      "aiohappyeyeballs                  2.6.1\n",
      "aiohttp                           3.13.2\n",
      "aiosignal                         1.4.0\n",
      "annotated-doc                     0.0.4\n",
      "annotated-types                   0.7.0\n",
      "anyio                             4.12.0\n",
      "anykeystore                       0.2\n",
      "asttokens                         3.0.0\n",
      "async-timeout                     4.0.3\n",
      "attrs                             25.4.0\n",
      "av                                14.4.0\n",
      "backoff                           2.2.1\n",
      "beautifulsoup4                    4.14.3\n",
      "beir                              2.2.0\n",
      "bitarray                          3.8.0\n",
      "certifi                           2025.11.12\n",
      "cffi                              2.0.0\n",
      "chardet                           5.2.0\n",
      "charset-normalizer                3.4.4\n",
      "click                             8.1.8\n",
      "cloudpickle                       3.1.2\n",
      "cmake                             4.2.1\n",
      "colpali_engine                    0.3.14.dev2+g06df5ce4f.d20260201 /home/ygm/llm-project/colpali\n",
      "comm                              0.2.3\n",
      "configue                          6.0.0\n",
      "contourpy                         1.3.0\n",
      "cryptacular                       1.6.2\n",
      "cryptography                      46.0.3\n",
      "cycler                            0.12.1\n",
      "dataclasses-json                  0.6.7\n",
      "datasets                          2.21.0\n",
      "debugpy                           1.8.17\n",
      "decorator                         5.2.1\n",
      "defusedxml                        0.7.1\n",
      "dill                              0.3.8\n",
      "diskcache                         5.6.3\n",
      "distro                            1.9.0\n",
      "easydict                          1.13\n",
      "einops                            0.8.1\n",
      "emoji                             2.15.0\n",
      "et_xmlfile                        2.0.0\n",
      "eval_type_backport                0.2.2\n",
      "exceptiongroup                    1.3.0\n",
      "executing                         2.2.1\n",
      "faiss-cpu                         1.9.0\n",
      "fastapi                           0.128.0\n",
      "fastkmeans                        0.5.0\n",
      "filelock                          3.19.1\n",
      "filetype                          1.2.0\n",
      "fire                              0.7.1\n",
      "flash-attn                        2.7.3\n",
      "fonttools                         4.60.2\n",
      "frozenlist                        1.8.0\n",
      "fsspec                            2024.6.1\n",
      "genson                            1.3.0\n",
      "gitdb                             4.0.12\n",
      "GitPython                         3.1.45\n",
      "GPUtil                            1.4.0\n",
      "greenlet                          1.1.2\n",
      "h11                               0.16.0\n",
      "hf_transfer                       0.1.9\n",
      "hf-xet                            1.2.0\n",
      "html5lib                          1.1\n",
      "httpcore                          1.0.9\n",
      "httptools                         0.7.1\n",
      "httpx                             0.28.1\n",
      "httpx-sse                         0.4.3\n",
      "huggingface-hub                   0.36.0\n",
      "hupper                            1.12.1\n",
      "idna                              3.11\n",
      "importlib_metadata                8.7.0\n",
      "importlib_resources               6.5.2\n",
      "iniconfig                         2.1.0\n",
      "interegular                       0.3.3\n",
      "ipykernel                         6.31.0\n",
      "ipython                           8.18.1\n",
      "jedi                              0.19.2\n",
      "Jinja2                            3.1.6\n",
      "jiter                             0.12.0\n",
      "joblib                            1.5.2\n",
      "jsonlines                         4.0.0\n",
      "jsonpatch                         1.33\n",
      "jsonpath-ng                       1.7.0\n",
      "jsonpointer                       3.0.0\n",
      "jsonschema                        4.25.1\n",
      "jsonschema-specifications         2025.9.1\n",
      "jupyter_client                    8.6.3\n",
      "jupyter_core                      5.8.1\n",
      "kiwisolver                        1.4.7\n",
      "langchain                         0.3.27\n",
      "langchain-community               0.3.31\n",
      "langchain-core                    0.3.81\n",
      "langchain-text-splitters          0.3.11\n",
      "langdetect                        1.0.9\n",
      "langsmith                         0.4.37\n",
      "Levenshtein                       0.27.1\n",
      "lm-format-enforcer                0.10.1\n",
      "loguru                            0.7.3\n",
      "lxml                              6.0.2\n",
      "markdown-it-py                    3.0.0\n",
      "MarkupSafe                        3.0.3\n",
      "marshmallow                       3.26.2\n",
      "matplotlib                        3.9.4\n",
      "matplotlib-inline                 0.2.1\n",
      "mdurl                             0.1.2\n",
      "mpmath                            1.3.0\n",
      "msgpack                           1.1.2\n",
      "mteb                              1.39.7\n",
      "multidict                         6.7.0\n",
      "multiprocess                      0.70.16\n",
      "mypy_extensions                   1.1.0\n",
      "nest-asyncio                      1.6.0\n",
      "networkx                          3.2.1\n",
      "ninja                             1.13.0\n",
      "nltk                              3.9.2\n",
      "numpy                             1.26.4\n",
      "nvidia-cublas-cu12                12.1.3.1\n",
      "nvidia-cuda-cupti-cu12            12.1.105\n",
      "nvidia-cuda-nvrtc-cu12            12.1.105\n",
      "nvidia-cuda-runtime-cu12          12.1.105\n",
      "nvidia-cudnn-cu12                 8.9.2.26\n",
      "nvidia-cufft-cu12                 11.0.2.54\n",
      "nvidia-curand-cu12                10.3.2.106\n",
      "nvidia-cusolver-cu12              11.4.5.107\n",
      "nvidia-cusparse-cu12              12.1.0.106\n",
      "nvidia-cusparselt-cu12            0.6.2\n",
      "nvidia-ml-py                      13.580.82\n",
      "nvidia-nccl-cu12                  2.20.5\n",
      "nvidia-nvjitlink-cu12             12.4.127\n",
      "nvidia-nvtx-cu12                  12.1.105\n",
      "nvitop                            1.6.1\n",
      "oauthlib                          3.3.1\n",
      "olefile                           0.47\n",
      "openai                            1.76.0\n",
      "opencv-python-headless            4.11.0.86\n",
      "openpyxl                          3.1.5\n",
      "orjson                            3.11.5\n",
      "outlines                          1.2.9\n",
      "outlines_core                     0.2.11\n",
      "packaging                         25.0\n",
      "pandas                            2.3.2\n",
      "parso                             0.8.5\n",
      "PasteDeploy                       3.1.0\n",
      "pbkdf2                            1.3\n",
      "pdf2image                         1.17.0\n",
      "pdfminer                          20191125\n",
      "pdfminer.six                      20251107\n",
      "pdfrw                             0.4\n",
      "peft                              0.14.0\n",
      "pexpect                           4.9.0\n",
      "pi_heif                           1.0.0\n",
      "pillow                            11.3.0\n",
      "pip                               25.2\n",
      "plaster                           1.1.2\n",
      "plaster-pastedeploy               1.0.1\n",
      "platformdirs                      4.4.0\n",
      "playwright                        1.19.0\n",
      "pluggy                            1.6.0\n",
      "ply                               3.11\n",
      "polars                            1.35.2\n",
      "polars-runtime-32                 1.35.2\n",
      "prometheus_client                 0.23.1\n",
      "prometheus-fastapi-instrumentator 7.1.0\n",
      "prompt_toolkit                    3.0.52\n",
      "propcache                         0.4.1\n",
      "protobuf                          6.33.0\n",
      "psutil                            7.1.3\n",
      "ptyprocess                        0.7.0\n",
      "pure_eval                         0.2.3\n",
      "py-cpuinfo                        9.0.0\n",
      "pyarrow                           20.0.0\n",
      "pycparser                         2.23\n",
      "pycryptodome                      3.23.0\n",
      "pydantic                          2.12.4\n",
      "pydantic_core                     2.41.5\n",
      "pydantic-settings                 2.11.0\n",
      "pyee                              8.1.0\n",
      "Pygments                          2.19.2\n",
      "pynvml                            13.0.1\n",
      "pyparsing                         3.3.1\n",
      "pypdf                             6.5.0\n",
      "pyramid                           2.0.2\n",
      "pyramid-mailer                    0.15.1\n",
      "pytest                            8.4.2\n",
      "python-dateutil                   2.9.0.post0\n",
      "python-dotenv                     1.2.1\n",
      "python-iso639                     2025.2.18\n",
      "python-Levenshtein                0.27.1\n",
      "python-magic                      0.4.27\n",
      "python-oxmsg                      0.0.2\n",
      "python3-openid                    3.2.0\n",
      "pytrec_eval                       0.5\n",
      "pytrec_eval-terrier               0.5.7\n",
      "pytz                              2025.2\n",
      "PyYAML                            6.0.3\n",
      "pyzmq                             27.1.0\n",
      "qwen-vl-utils                     0.0.14\n",
      "RapidFuzz                         3.13.0\n",
      "ray                               2.51.2\n",
      "referencing                       0.36.2\n",
      "regex                             2025.11.3\n",
      "repoze.sendmail                   4.4.1\n",
      "requests                          2.32.5\n",
      "requests-oauthlib                 2.0.0\n",
      "requests-toolbelt                 1.0.0\n",
      "rich                              14.2.0\n",
      "rpds-py                           0.27.1\n",
      "safetensors                       0.6.2\n",
      "scikit-learn                      1.6.1\n",
      "scipy                             1.13.1\n",
      "seaborn                           0.13.2\n",
      "sentence-transformers             3.4.1\n",
      "sentencepiece                     0.2.0\n",
      "sentry-sdk                        2.44.0\n",
      "setuptools                        80.9.0\n",
      "shellingham                       1.5.4\n",
      "six                               1.17.0\n",
      "smmap                             5.0.2\n",
      "sniffio                           1.3.1\n",
      "soupsieve                         2.8.1\n",
      "SQLAlchemy                        2.0.45\n",
      "stack-data                        0.6.3\n",
      "starlette                         0.49.3\n",
      "sympy                             1.13.1\n",
      "tenacity                          9.1.2\n",
      "termcolor                         3.1.0\n",
      "threadpoolctl                     3.6.0\n",
      "tiktoken                          0.11.0\n",
      "timm                              1.0.15\n",
      "tokenizers                        0.22.2\n",
      "tomli                             2.3.0\n",
      "torch                             2.3.0\n",
      "torchvision                       0.18.0\n",
      "tornado                           6.5.2\n",
      "tqdm                              4.67.1\n",
      "traitlets                         5.14.3\n",
      "transaction                       5.0\n",
      "transformers                      4.57.1\n",
      "translationstring                 1.4\n",
      "triton                            2.3.0\n",
      "typer                             0.20.0\n",
      "typing_extensions                 4.15.0\n",
      "typing-inspect                    0.9.0\n",
      "typing-inspection                 0.4.2\n",
      "tzdata                            2025.2\n",
      "unstructured                      0.18.3\n",
      "unstructured-client               0.42.6\n",
      "urllib3                           2.5.0\n",
      "uv                                0.9.18\n",
      "uvicorn                           0.39.0\n",
      "uvloop                            0.22.1\n",
      "velruse                           1.1.1\n",
      "venusian                          3.1.1\n",
      "vllm                              0.5.1\n",
      "vllm-flash-attn                   2.5.9\n",
      "wandb                             0.23.0\n",
      "watchfiles                        1.1.1\n",
      "wcwidth                           0.2.14\n",
      "webencodings                      0.5.1\n",
      "WebOb                             1.8.9\n",
      "websockets                        15.0.1\n",
      "wheel                             0.45.1\n",
      "wrapt                             2.0.1\n",
      "WTForms                           3.2.1\n",
      "wtforms-recaptcha                 0.3.2\n",
      "xformers                          0.0.26.post1\n",
      "xxhash                            3.6.0\n",
      "yarl                              1.22.0\n",
      "zipp                              3.23.0\n",
      "zope.deprecation                  6.0\n",
      "zope.interface                    8.0.1\n",
      "zope.sqlalchemy                   4.0\n",
      "zstandard                         0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen3VLForConditionalGeneration,AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    \"/mnt/data1/ygm/models/Qwen3-VL-2B-Instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3VLForConditionalGeneration(\n",
       "  (model): Qwen3VLModel(\n",
       "    (visual): Qwen3VLVisionModel(\n",
       "      (patch_embed): Qwen3VLVisionPatchEmbed(\n",
       "        (proj): Conv3d(3, 1024, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "      (pos_embed): Embedding(2304, 1024)\n",
       "      (rotary_pos_emb): Qwen3VLVisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x Qwen3VLVisionBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Qwen3VLVisionAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): Qwen3VLVisionMLP(\n",
       "            (linear_fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (linear_fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (act_fn): GELUTanh()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): Qwen3VLVisionPatchMerger(\n",
       "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (act_fn): GELU(approximate='none')\n",
       "        (linear_fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "      )\n",
       "      (deepstack_merger_list): ModuleList(\n",
       "        (0-2): 3 x Qwen3VLVisionPatchMerger(\n",
       "          (norm): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "          (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (act_fn): GELU(approximate='none')\n",
       "          (linear_fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): Qwen3VLTextModel(\n",
       "      (embed_tokens): Embedding(151936, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen3VLTextDecoderLayer(\n",
       "          (self_attn): Qwen3VLTextAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
       "            (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Qwen3VLTextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "            (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): Qwen3VLTextRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3VLConfig {\n",
       "  \"architectures\": [\n",
       "    \"Qwen3VLForConditionalGeneration\"\n",
       "  ],\n",
       "  \"dtype\": \"float32\",\n",
       "  \"image_token_id\": 151655,\n",
       "  \"model_type\": \"qwen3_vl\",\n",
       "  \"text_config\": {\n",
       "    \"attention_bias\": false,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bos_token_id\": 151643,\n",
       "    \"dtype\": \"float32\",\n",
       "    \"eos_token_id\": 151645,\n",
       "    \"head_dim\": 128,\n",
       "    \"hidden_act\": \"silu\",\n",
       "    \"hidden_size\": 2048,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 6144,\n",
       "    \"max_position_embeddings\": 262144,\n",
       "    \"model_type\": \"qwen3_vl_text\",\n",
       "    \"num_attention_heads\": 16,\n",
       "    \"num_hidden_layers\": 28,\n",
       "    \"num_key_value_heads\": 8,\n",
       "    \"rms_norm_eps\": 1e-06,\n",
       "    \"rope_scaling\": {\n",
       "      \"mrope_interleaved\": true,\n",
       "      \"mrope_section\": [\n",
       "        24,\n",
       "        20,\n",
       "        20\n",
       "      ],\n",
       "      \"rope_type\": \"default\"\n",
       "    },\n",
       "    \"rope_theta\": 5000000,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 151936\n",
       "  },\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"transformers_version\": \"4.57.1\",\n",
       "  \"video_token_id\": 151656,\n",
       "  \"vision_config\": {\n",
       "    \"deepstack_visual_indexes\": [\n",
       "      5,\n",
       "      11,\n",
       "      17\n",
       "    ],\n",
       "    \"depth\": 24,\n",
       "    \"dtype\": \"float32\",\n",
       "    \"hidden_act\": \"gelu_pytorch_tanh\",\n",
       "    \"hidden_size\": 1024,\n",
       "    \"in_channels\": 3,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 4096,\n",
       "    \"model_type\": \"qwen3_vl\",\n",
       "    \"num_heads\": 16,\n",
       "    \"num_position_embeddings\": 2304,\n",
       "    \"out_hidden_size\": 2048,\n",
       "    \"patch_size\": 16,\n",
       "    \"spatial_merge_size\": 2,\n",
       "    \"temporal_patch_size\": 2\n",
       "  },\n",
       "  \"vision_end_token_id\": 151653,\n",
       "  \"vision_start_token_id\": 151652\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"/mnt/data1/ygm/models/Qwen3-VL-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3VLProcessor:\n",
       "- image_processor: Qwen2VLImageProcessorFast {\n",
       "  \"crop_size\": null,\n",
       "  \"data_format\": \"channels_first\",\n",
       "  \"default_to_square\": true,\n",
       "  \"device\": null,\n",
       "  \"disable_grouping\": null,\n",
       "  \"do_center_crop\": null,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_pad\": null,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"Qwen2VLImageProcessorFast\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"input_data_format\": null,\n",
       "  \"max_pixels\": null,\n",
       "  \"merge_size\": 2,\n",
       "  \"min_pixels\": null,\n",
       "  \"pad_size\": null,\n",
       "  \"patch_size\": 16,\n",
       "  \"processor_class\": \"Qwen3VLProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"return_tensors\": null,\n",
       "  \"size\": {\n",
       "    \"longest_edge\": 16777216,\n",
       "    \"shortest_edge\": 65536\n",
       "  },\n",
       "  \"temporal_patch_size\": 2\n",
       "}\n",
       "\n",
       "- tokenizer: Qwen2TokenizerFast(name_or_path='/mnt/data1/ygm/models/Qwen3-VL-2B-Instruct', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")\n",
       "- video_processor: Qwen3VLVideoProcessor {\n",
       "  \"crop_size\": null,\n",
       "  \"data_format\": \"channels_first\",\n",
       "  \"default_to_square\": true,\n",
       "  \"device\": null,\n",
       "  \"do_center_crop\": null,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"do_sample_frames\": true,\n",
       "  \"fps\": 2,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"input_data_format\": null,\n",
       "  \"max_frames\": 768,\n",
       "  \"merge_size\": 2,\n",
       "  \"min_frames\": 4,\n",
       "  \"num_frames\": null,\n",
       "  \"pad_size\": null,\n",
       "  \"patch_size\": 16,\n",
       "  \"processor_class\": \"Qwen3VLProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"return_metadata\": false,\n",
       "  \"size\": {\n",
       "    \"longest_edge\": 25165824,\n",
       "    \"shortest_edge\": 4096\n",
       "  },\n",
       "  \"temporal_patch_size\": 2,\n",
       "  \"video_metadata\": null,\n",
       "  \"video_processor_type\": \"Qwen3VLVideoProcessor\"\n",
       "}\n",
       "\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"Qwen3VLProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosel Size: 2127.53M parameters\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print(f\"Mosel Size: {total_params / 1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.visual.patch_embed.proj.weight\n",
      "model.visual.patch_embed.proj.bias\n",
      "model.visual.pos_embed.weight\n",
      "model.visual.blocks.0.norm1.weight\n",
      "model.visual.blocks.0.norm1.bias\n",
      "model.visual.blocks.0.norm2.weight\n",
      "model.visual.blocks.0.norm2.bias\n",
      "model.visual.blocks.0.attn.qkv.weight\n",
      "model.visual.blocks.0.attn.qkv.bias\n",
      "model.visual.blocks.0.attn.proj.weight\n",
      "model.visual.blocks.0.attn.proj.bias\n",
      "model.visual.blocks.0.mlp.linear_fc1.weight\n",
      "model.visual.blocks.0.mlp.linear_fc1.bias\n",
      "model.visual.blocks.0.mlp.linear_fc2.weight\n",
      "model.visual.blocks.0.mlp.linear_fc2.bias\n",
      "model.visual.blocks.1.norm1.weight\n",
      "model.visual.blocks.1.norm1.bias\n",
      "model.visual.blocks.1.norm2.weight\n",
      "model.visual.blocks.1.norm2.bias\n",
      "model.visual.blocks.1.attn.qkv.weight\n",
      "model.visual.blocks.1.attn.qkv.bias\n",
      "model.visual.blocks.1.attn.proj.weight\n",
      "model.visual.blocks.1.attn.proj.bias\n",
      "model.visual.blocks.1.mlp.linear_fc1.weight\n",
      "model.visual.blocks.1.mlp.linear_fc1.bias\n",
      "model.visual.blocks.1.mlp.linear_fc2.weight\n",
      "model.visual.blocks.1.mlp.linear_fc2.bias\n",
      "model.visual.blocks.2.norm1.weight\n",
      "model.visual.blocks.2.norm1.bias\n",
      "model.visual.blocks.2.norm2.weight\n",
      "model.visual.blocks.2.norm2.bias\n",
      "model.visual.blocks.2.attn.qkv.weight\n",
      "model.visual.blocks.2.attn.qkv.bias\n",
      "model.visual.blocks.2.attn.proj.weight\n",
      "model.visual.blocks.2.attn.proj.bias\n",
      "model.visual.blocks.2.mlp.linear_fc1.weight\n",
      "model.visual.blocks.2.mlp.linear_fc1.bias\n",
      "model.visual.blocks.2.mlp.linear_fc2.weight\n",
      "model.visual.blocks.2.mlp.linear_fc2.bias\n",
      "model.visual.blocks.3.norm1.weight\n",
      "model.visual.blocks.3.norm1.bias\n",
      "model.visual.blocks.3.norm2.weight\n",
      "model.visual.blocks.3.norm2.bias\n",
      "model.visual.blocks.3.attn.qkv.weight\n",
      "model.visual.blocks.3.attn.qkv.bias\n",
      "model.visual.blocks.3.attn.proj.weight\n",
      "model.visual.blocks.3.attn.proj.bias\n",
      "model.visual.blocks.3.mlp.linear_fc1.weight\n",
      "model.visual.blocks.3.mlp.linear_fc1.bias\n",
      "model.visual.blocks.3.mlp.linear_fc2.weight\n",
      "model.visual.blocks.3.mlp.linear_fc2.bias\n",
      "model.visual.blocks.4.norm1.weight\n",
      "model.visual.blocks.4.norm1.bias\n",
      "model.visual.blocks.4.norm2.weight\n",
      "model.visual.blocks.4.norm2.bias\n",
      "model.visual.blocks.4.attn.qkv.weight\n",
      "model.visual.blocks.4.attn.qkv.bias\n",
      "model.visual.blocks.4.attn.proj.weight\n",
      "model.visual.blocks.4.attn.proj.bias\n",
      "model.visual.blocks.4.mlp.linear_fc1.weight\n",
      "model.visual.blocks.4.mlp.linear_fc1.bias\n",
      "model.visual.blocks.4.mlp.linear_fc2.weight\n",
      "model.visual.blocks.4.mlp.linear_fc2.bias\n",
      "model.visual.blocks.5.norm1.weight\n",
      "model.visual.blocks.5.norm1.bias\n",
      "model.visual.blocks.5.norm2.weight\n",
      "model.visual.blocks.5.norm2.bias\n",
      "model.visual.blocks.5.attn.qkv.weight\n",
      "model.visual.blocks.5.attn.qkv.bias\n",
      "model.visual.blocks.5.attn.proj.weight\n",
      "model.visual.blocks.5.attn.proj.bias\n",
      "model.visual.blocks.5.mlp.linear_fc1.weight\n",
      "model.visual.blocks.5.mlp.linear_fc1.bias\n",
      "model.visual.blocks.5.mlp.linear_fc2.weight\n",
      "model.visual.blocks.5.mlp.linear_fc2.bias\n",
      "model.visual.blocks.6.norm1.weight\n",
      "model.visual.blocks.6.norm1.bias\n",
      "model.visual.blocks.6.norm2.weight\n",
      "model.visual.blocks.6.norm2.bias\n",
      "model.visual.blocks.6.attn.qkv.weight\n",
      "model.visual.blocks.6.attn.qkv.bias\n",
      "model.visual.blocks.6.attn.proj.weight\n",
      "model.visual.blocks.6.attn.proj.bias\n",
      "model.visual.blocks.6.mlp.linear_fc1.weight\n",
      "model.visual.blocks.6.mlp.linear_fc1.bias\n",
      "model.visual.blocks.6.mlp.linear_fc2.weight\n",
      "model.visual.blocks.6.mlp.linear_fc2.bias\n",
      "model.visual.blocks.7.norm1.weight\n",
      "model.visual.blocks.7.norm1.bias\n",
      "model.visual.blocks.7.norm2.weight\n",
      "model.visual.blocks.7.norm2.bias\n",
      "model.visual.blocks.7.attn.qkv.weight\n",
      "model.visual.blocks.7.attn.qkv.bias\n",
      "model.visual.blocks.7.attn.proj.weight\n",
      "model.visual.blocks.7.attn.proj.bias\n",
      "model.visual.blocks.7.mlp.linear_fc1.weight\n",
      "model.visual.blocks.7.mlp.linear_fc1.bias\n",
      "model.visual.blocks.7.mlp.linear_fc2.weight\n",
      "model.visual.blocks.7.mlp.linear_fc2.bias\n",
      "model.visual.blocks.8.norm1.weight\n",
      "model.visual.blocks.8.norm1.bias\n",
      "model.visual.blocks.8.norm2.weight\n",
      "model.visual.blocks.8.norm2.bias\n",
      "model.visual.blocks.8.attn.qkv.weight\n",
      "model.visual.blocks.8.attn.qkv.bias\n",
      "model.visual.blocks.8.attn.proj.weight\n",
      "model.visual.blocks.8.attn.proj.bias\n",
      "model.visual.blocks.8.mlp.linear_fc1.weight\n",
      "model.visual.blocks.8.mlp.linear_fc1.bias\n",
      "model.visual.blocks.8.mlp.linear_fc2.weight\n",
      "model.visual.blocks.8.mlp.linear_fc2.bias\n",
      "model.visual.blocks.9.norm1.weight\n",
      "model.visual.blocks.9.norm1.bias\n",
      "model.visual.blocks.9.norm2.weight\n",
      "model.visual.blocks.9.norm2.bias\n",
      "model.visual.blocks.9.attn.qkv.weight\n",
      "model.visual.blocks.9.attn.qkv.bias\n",
      "model.visual.blocks.9.attn.proj.weight\n",
      "model.visual.blocks.9.attn.proj.bias\n",
      "model.visual.blocks.9.mlp.linear_fc1.weight\n",
      "model.visual.blocks.9.mlp.linear_fc1.bias\n",
      "model.visual.blocks.9.mlp.linear_fc2.weight\n",
      "model.visual.blocks.9.mlp.linear_fc2.bias\n",
      "model.visual.blocks.10.norm1.weight\n",
      "model.visual.blocks.10.norm1.bias\n",
      "model.visual.blocks.10.norm2.weight\n",
      "model.visual.blocks.10.norm2.bias\n",
      "model.visual.blocks.10.attn.qkv.weight\n",
      "model.visual.blocks.10.attn.qkv.bias\n",
      "model.visual.blocks.10.attn.proj.weight\n",
      "model.visual.blocks.10.attn.proj.bias\n",
      "model.visual.blocks.10.mlp.linear_fc1.weight\n",
      "model.visual.blocks.10.mlp.linear_fc1.bias\n",
      "model.visual.blocks.10.mlp.linear_fc2.weight\n",
      "model.visual.blocks.10.mlp.linear_fc2.bias\n",
      "model.visual.blocks.11.norm1.weight\n",
      "model.visual.blocks.11.norm1.bias\n",
      "model.visual.blocks.11.norm2.weight\n",
      "model.visual.blocks.11.norm2.bias\n",
      "model.visual.blocks.11.attn.qkv.weight\n",
      "model.visual.blocks.11.attn.qkv.bias\n",
      "model.visual.blocks.11.attn.proj.weight\n",
      "model.visual.blocks.11.attn.proj.bias\n",
      "model.visual.blocks.11.mlp.linear_fc1.weight\n",
      "model.visual.blocks.11.mlp.linear_fc1.bias\n",
      "model.visual.blocks.11.mlp.linear_fc2.weight\n",
      "model.visual.blocks.11.mlp.linear_fc2.bias\n",
      "model.visual.blocks.12.norm1.weight\n",
      "model.visual.blocks.12.norm1.bias\n",
      "model.visual.blocks.12.norm2.weight\n",
      "model.visual.blocks.12.norm2.bias\n",
      "model.visual.blocks.12.attn.qkv.weight\n",
      "model.visual.blocks.12.attn.qkv.bias\n",
      "model.visual.blocks.12.attn.proj.weight\n",
      "model.visual.blocks.12.attn.proj.bias\n",
      "model.visual.blocks.12.mlp.linear_fc1.weight\n",
      "model.visual.blocks.12.mlp.linear_fc1.bias\n",
      "model.visual.blocks.12.mlp.linear_fc2.weight\n",
      "model.visual.blocks.12.mlp.linear_fc2.bias\n",
      "model.visual.blocks.13.norm1.weight\n",
      "model.visual.blocks.13.norm1.bias\n",
      "model.visual.blocks.13.norm2.weight\n",
      "model.visual.blocks.13.norm2.bias\n",
      "model.visual.blocks.13.attn.qkv.weight\n",
      "model.visual.blocks.13.attn.qkv.bias\n",
      "model.visual.blocks.13.attn.proj.weight\n",
      "model.visual.blocks.13.attn.proj.bias\n",
      "model.visual.blocks.13.mlp.linear_fc1.weight\n",
      "model.visual.blocks.13.mlp.linear_fc1.bias\n",
      "model.visual.blocks.13.mlp.linear_fc2.weight\n",
      "model.visual.blocks.13.mlp.linear_fc2.bias\n",
      "model.visual.blocks.14.norm1.weight\n",
      "model.visual.blocks.14.norm1.bias\n",
      "model.visual.blocks.14.norm2.weight\n",
      "model.visual.blocks.14.norm2.bias\n",
      "model.visual.blocks.14.attn.qkv.weight\n",
      "model.visual.blocks.14.attn.qkv.bias\n",
      "model.visual.blocks.14.attn.proj.weight\n",
      "model.visual.blocks.14.attn.proj.bias\n",
      "model.visual.blocks.14.mlp.linear_fc1.weight\n",
      "model.visual.blocks.14.mlp.linear_fc1.bias\n",
      "model.visual.blocks.14.mlp.linear_fc2.weight\n",
      "model.visual.blocks.14.mlp.linear_fc2.bias\n",
      "model.visual.blocks.15.norm1.weight\n",
      "model.visual.blocks.15.norm1.bias\n",
      "model.visual.blocks.15.norm2.weight\n",
      "model.visual.blocks.15.norm2.bias\n",
      "model.visual.blocks.15.attn.qkv.weight\n",
      "model.visual.blocks.15.attn.qkv.bias\n",
      "model.visual.blocks.15.attn.proj.weight\n",
      "model.visual.blocks.15.attn.proj.bias\n",
      "model.visual.blocks.15.mlp.linear_fc1.weight\n",
      "model.visual.blocks.15.mlp.linear_fc1.bias\n",
      "model.visual.blocks.15.mlp.linear_fc2.weight\n",
      "model.visual.blocks.15.mlp.linear_fc2.bias\n",
      "model.visual.blocks.16.norm1.weight\n",
      "model.visual.blocks.16.norm1.bias\n",
      "model.visual.blocks.16.norm2.weight\n",
      "model.visual.blocks.16.norm2.bias\n",
      "model.visual.blocks.16.attn.qkv.weight\n",
      "model.visual.blocks.16.attn.qkv.bias\n",
      "model.visual.blocks.16.attn.proj.weight\n",
      "model.visual.blocks.16.attn.proj.bias\n",
      "model.visual.blocks.16.mlp.linear_fc1.weight\n",
      "model.visual.blocks.16.mlp.linear_fc1.bias\n",
      "model.visual.blocks.16.mlp.linear_fc2.weight\n",
      "model.visual.blocks.16.mlp.linear_fc2.bias\n",
      "model.visual.blocks.17.norm1.weight\n",
      "model.visual.blocks.17.norm1.bias\n",
      "model.visual.blocks.17.norm2.weight\n",
      "model.visual.blocks.17.norm2.bias\n",
      "model.visual.blocks.17.attn.qkv.weight\n",
      "model.visual.blocks.17.attn.qkv.bias\n",
      "model.visual.blocks.17.attn.proj.weight\n",
      "model.visual.blocks.17.attn.proj.bias\n",
      "model.visual.blocks.17.mlp.linear_fc1.weight\n",
      "model.visual.blocks.17.mlp.linear_fc1.bias\n",
      "model.visual.blocks.17.mlp.linear_fc2.weight\n",
      "model.visual.blocks.17.mlp.linear_fc2.bias\n",
      "model.visual.blocks.18.norm1.weight\n",
      "model.visual.blocks.18.norm1.bias\n",
      "model.visual.blocks.18.norm2.weight\n",
      "model.visual.blocks.18.norm2.bias\n",
      "model.visual.blocks.18.attn.qkv.weight\n",
      "model.visual.blocks.18.attn.qkv.bias\n",
      "model.visual.blocks.18.attn.proj.weight\n",
      "model.visual.blocks.18.attn.proj.bias\n",
      "model.visual.blocks.18.mlp.linear_fc1.weight\n",
      "model.visual.blocks.18.mlp.linear_fc1.bias\n",
      "model.visual.blocks.18.mlp.linear_fc2.weight\n",
      "model.visual.blocks.18.mlp.linear_fc2.bias\n",
      "model.visual.blocks.19.norm1.weight\n",
      "model.visual.blocks.19.norm1.bias\n",
      "model.visual.blocks.19.norm2.weight\n",
      "model.visual.blocks.19.norm2.bias\n",
      "model.visual.blocks.19.attn.qkv.weight\n",
      "model.visual.blocks.19.attn.qkv.bias\n",
      "model.visual.blocks.19.attn.proj.weight\n",
      "model.visual.blocks.19.attn.proj.bias\n",
      "model.visual.blocks.19.mlp.linear_fc1.weight\n",
      "model.visual.blocks.19.mlp.linear_fc1.bias\n",
      "model.visual.blocks.19.mlp.linear_fc2.weight\n",
      "model.visual.blocks.19.mlp.linear_fc2.bias\n",
      "model.visual.blocks.20.norm1.weight\n",
      "model.visual.blocks.20.norm1.bias\n",
      "model.visual.blocks.20.norm2.weight\n",
      "model.visual.blocks.20.norm2.bias\n",
      "model.visual.blocks.20.attn.qkv.weight\n",
      "model.visual.blocks.20.attn.qkv.bias\n",
      "model.visual.blocks.20.attn.proj.weight\n",
      "model.visual.blocks.20.attn.proj.bias\n",
      "model.visual.blocks.20.mlp.linear_fc1.weight\n",
      "model.visual.blocks.20.mlp.linear_fc1.bias\n",
      "model.visual.blocks.20.mlp.linear_fc2.weight\n",
      "model.visual.blocks.20.mlp.linear_fc2.bias\n",
      "model.visual.blocks.21.norm1.weight\n",
      "model.visual.blocks.21.norm1.bias\n",
      "model.visual.blocks.21.norm2.weight\n",
      "model.visual.blocks.21.norm2.bias\n",
      "model.visual.blocks.21.attn.qkv.weight\n",
      "model.visual.blocks.21.attn.qkv.bias\n",
      "model.visual.blocks.21.attn.proj.weight\n",
      "model.visual.blocks.21.attn.proj.bias\n",
      "model.visual.blocks.21.mlp.linear_fc1.weight\n",
      "model.visual.blocks.21.mlp.linear_fc1.bias\n",
      "model.visual.blocks.21.mlp.linear_fc2.weight\n",
      "model.visual.blocks.21.mlp.linear_fc2.bias\n",
      "model.visual.blocks.22.norm1.weight\n",
      "model.visual.blocks.22.norm1.bias\n",
      "model.visual.blocks.22.norm2.weight\n",
      "model.visual.blocks.22.norm2.bias\n",
      "model.visual.blocks.22.attn.qkv.weight\n",
      "model.visual.blocks.22.attn.qkv.bias\n",
      "model.visual.blocks.22.attn.proj.weight\n",
      "model.visual.blocks.22.attn.proj.bias\n",
      "model.visual.blocks.22.mlp.linear_fc1.weight\n",
      "model.visual.blocks.22.mlp.linear_fc1.bias\n",
      "model.visual.blocks.22.mlp.linear_fc2.weight\n",
      "model.visual.blocks.22.mlp.linear_fc2.bias\n",
      "model.visual.blocks.23.norm1.weight\n",
      "model.visual.blocks.23.norm1.bias\n",
      "model.visual.blocks.23.norm2.weight\n",
      "model.visual.blocks.23.norm2.bias\n",
      "model.visual.blocks.23.attn.qkv.weight\n",
      "model.visual.blocks.23.attn.qkv.bias\n",
      "model.visual.blocks.23.attn.proj.weight\n",
      "model.visual.blocks.23.attn.proj.bias\n",
      "model.visual.blocks.23.mlp.linear_fc1.weight\n",
      "model.visual.blocks.23.mlp.linear_fc1.bias\n",
      "model.visual.blocks.23.mlp.linear_fc2.weight\n",
      "model.visual.blocks.23.mlp.linear_fc2.bias\n",
      "model.visual.merger.norm.weight\n",
      "model.visual.merger.norm.bias\n",
      "model.visual.merger.linear_fc1.weight\n",
      "model.visual.merger.linear_fc1.bias\n",
      "model.visual.merger.linear_fc2.weight\n",
      "model.visual.merger.linear_fc2.bias\n",
      "model.visual.deepstack_merger_list.0.norm.weight\n",
      "model.visual.deepstack_merger_list.0.norm.bias\n",
      "model.visual.deepstack_merger_list.0.linear_fc1.weight\n",
      "model.visual.deepstack_merger_list.0.linear_fc1.bias\n",
      "model.visual.deepstack_merger_list.0.linear_fc2.weight\n",
      "model.visual.deepstack_merger_list.0.linear_fc2.bias\n",
      "model.visual.deepstack_merger_list.1.norm.weight\n",
      "model.visual.deepstack_merger_list.1.norm.bias\n",
      "model.visual.deepstack_merger_list.1.linear_fc1.weight\n",
      "model.visual.deepstack_merger_list.1.linear_fc1.bias\n",
      "model.visual.deepstack_merger_list.1.linear_fc2.weight\n",
      "model.visual.deepstack_merger_list.1.linear_fc2.bias\n",
      "model.visual.deepstack_merger_list.2.norm.weight\n",
      "model.visual.deepstack_merger_list.2.norm.bias\n",
      "model.visual.deepstack_merger_list.2.linear_fc1.weight\n",
      "model.visual.deepstack_merger_list.2.linear_fc1.bias\n",
      "model.visual.deepstack_merger_list.2.linear_fc2.weight\n",
      "model.visual.deepstack_merger_list.2.linear_fc2.bias\n",
      "model.language_model.embed_tokens.weight\n",
      "model.language_model.layers.0.self_attn.q_proj.weight\n",
      "model.language_model.layers.0.self_attn.k_proj.weight\n",
      "model.language_model.layers.0.self_attn.v_proj.weight\n",
      "model.language_model.layers.0.self_attn.o_proj.weight\n",
      "model.language_model.layers.0.self_attn.q_norm.weight\n",
      "model.language_model.layers.0.self_attn.k_norm.weight\n",
      "model.language_model.layers.0.mlp.gate_proj.weight\n",
      "model.language_model.layers.0.mlp.up_proj.weight\n",
      "model.language_model.layers.0.mlp.down_proj.weight\n",
      "model.language_model.layers.0.input_layernorm.weight\n",
      "model.language_model.layers.0.post_attention_layernorm.weight\n",
      "model.language_model.layers.1.self_attn.q_proj.weight\n",
      "model.language_model.layers.1.self_attn.k_proj.weight\n",
      "model.language_model.layers.1.self_attn.v_proj.weight\n",
      "model.language_model.layers.1.self_attn.o_proj.weight\n",
      "model.language_model.layers.1.self_attn.q_norm.weight\n",
      "model.language_model.layers.1.self_attn.k_norm.weight\n",
      "model.language_model.layers.1.mlp.gate_proj.weight\n",
      "model.language_model.layers.1.mlp.up_proj.weight\n",
      "model.language_model.layers.1.mlp.down_proj.weight\n",
      "model.language_model.layers.1.input_layernorm.weight\n",
      "model.language_model.layers.1.post_attention_layernorm.weight\n",
      "model.language_model.layers.2.self_attn.q_proj.weight\n",
      "model.language_model.layers.2.self_attn.k_proj.weight\n",
      "model.language_model.layers.2.self_attn.v_proj.weight\n",
      "model.language_model.layers.2.self_attn.o_proj.weight\n",
      "model.language_model.layers.2.self_attn.q_norm.weight\n",
      "model.language_model.layers.2.self_attn.k_norm.weight\n",
      "model.language_model.layers.2.mlp.gate_proj.weight\n",
      "model.language_model.layers.2.mlp.up_proj.weight\n",
      "model.language_model.layers.2.mlp.down_proj.weight\n",
      "model.language_model.layers.2.input_layernorm.weight\n",
      "model.language_model.layers.2.post_attention_layernorm.weight\n",
      "model.language_model.layers.3.self_attn.q_proj.weight\n",
      "model.language_model.layers.3.self_attn.k_proj.weight\n",
      "model.language_model.layers.3.self_attn.v_proj.weight\n",
      "model.language_model.layers.3.self_attn.o_proj.weight\n",
      "model.language_model.layers.3.self_attn.q_norm.weight\n",
      "model.language_model.layers.3.self_attn.k_norm.weight\n",
      "model.language_model.layers.3.mlp.gate_proj.weight\n",
      "model.language_model.layers.3.mlp.up_proj.weight\n",
      "model.language_model.layers.3.mlp.down_proj.weight\n",
      "model.language_model.layers.3.input_layernorm.weight\n",
      "model.language_model.layers.3.post_attention_layernorm.weight\n",
      "model.language_model.layers.4.self_attn.q_proj.weight\n",
      "model.language_model.layers.4.self_attn.k_proj.weight\n",
      "model.language_model.layers.4.self_attn.v_proj.weight\n",
      "model.language_model.layers.4.self_attn.o_proj.weight\n",
      "model.language_model.layers.4.self_attn.q_norm.weight\n",
      "model.language_model.layers.4.self_attn.k_norm.weight\n",
      "model.language_model.layers.4.mlp.gate_proj.weight\n",
      "model.language_model.layers.4.mlp.up_proj.weight\n",
      "model.language_model.layers.4.mlp.down_proj.weight\n",
      "model.language_model.layers.4.input_layernorm.weight\n",
      "model.language_model.layers.4.post_attention_layernorm.weight\n",
      "model.language_model.layers.5.self_attn.q_proj.weight\n",
      "model.language_model.layers.5.self_attn.k_proj.weight\n",
      "model.language_model.layers.5.self_attn.v_proj.weight\n",
      "model.language_model.layers.5.self_attn.o_proj.weight\n",
      "model.language_model.layers.5.self_attn.q_norm.weight\n",
      "model.language_model.layers.5.self_attn.k_norm.weight\n",
      "model.language_model.layers.5.mlp.gate_proj.weight\n",
      "model.language_model.layers.5.mlp.up_proj.weight\n",
      "model.language_model.layers.5.mlp.down_proj.weight\n",
      "model.language_model.layers.5.input_layernorm.weight\n",
      "model.language_model.layers.5.post_attention_layernorm.weight\n",
      "model.language_model.layers.6.self_attn.q_proj.weight\n",
      "model.language_model.layers.6.self_attn.k_proj.weight\n",
      "model.language_model.layers.6.self_attn.v_proj.weight\n",
      "model.language_model.layers.6.self_attn.o_proj.weight\n",
      "model.language_model.layers.6.self_attn.q_norm.weight\n",
      "model.language_model.layers.6.self_attn.k_norm.weight\n",
      "model.language_model.layers.6.mlp.gate_proj.weight\n",
      "model.language_model.layers.6.mlp.up_proj.weight\n",
      "model.language_model.layers.6.mlp.down_proj.weight\n",
      "model.language_model.layers.6.input_layernorm.weight\n",
      "model.language_model.layers.6.post_attention_layernorm.weight\n",
      "model.language_model.layers.7.self_attn.q_proj.weight\n",
      "model.language_model.layers.7.self_attn.k_proj.weight\n",
      "model.language_model.layers.7.self_attn.v_proj.weight\n",
      "model.language_model.layers.7.self_attn.o_proj.weight\n",
      "model.language_model.layers.7.self_attn.q_norm.weight\n",
      "model.language_model.layers.7.self_attn.k_norm.weight\n",
      "model.language_model.layers.7.mlp.gate_proj.weight\n",
      "model.language_model.layers.7.mlp.up_proj.weight\n",
      "model.language_model.layers.7.mlp.down_proj.weight\n",
      "model.language_model.layers.7.input_layernorm.weight\n",
      "model.language_model.layers.7.post_attention_layernorm.weight\n",
      "model.language_model.layers.8.self_attn.q_proj.weight\n",
      "model.language_model.layers.8.self_attn.k_proj.weight\n",
      "model.language_model.layers.8.self_attn.v_proj.weight\n",
      "model.language_model.layers.8.self_attn.o_proj.weight\n",
      "model.language_model.layers.8.self_attn.q_norm.weight\n",
      "model.language_model.layers.8.self_attn.k_norm.weight\n",
      "model.language_model.layers.8.mlp.gate_proj.weight\n",
      "model.language_model.layers.8.mlp.up_proj.weight\n",
      "model.language_model.layers.8.mlp.down_proj.weight\n",
      "model.language_model.layers.8.input_layernorm.weight\n",
      "model.language_model.layers.8.post_attention_layernorm.weight\n",
      "model.language_model.layers.9.self_attn.q_proj.weight\n",
      "model.language_model.layers.9.self_attn.k_proj.weight\n",
      "model.language_model.layers.9.self_attn.v_proj.weight\n",
      "model.language_model.layers.9.self_attn.o_proj.weight\n",
      "model.language_model.layers.9.self_attn.q_norm.weight\n",
      "model.language_model.layers.9.self_attn.k_norm.weight\n",
      "model.language_model.layers.9.mlp.gate_proj.weight\n",
      "model.language_model.layers.9.mlp.up_proj.weight\n",
      "model.language_model.layers.9.mlp.down_proj.weight\n",
      "model.language_model.layers.9.input_layernorm.weight\n",
      "model.language_model.layers.9.post_attention_layernorm.weight\n",
      "model.language_model.layers.10.self_attn.q_proj.weight\n",
      "model.language_model.layers.10.self_attn.k_proj.weight\n",
      "model.language_model.layers.10.self_attn.v_proj.weight\n",
      "model.language_model.layers.10.self_attn.o_proj.weight\n",
      "model.language_model.layers.10.self_attn.q_norm.weight\n",
      "model.language_model.layers.10.self_attn.k_norm.weight\n",
      "model.language_model.layers.10.mlp.gate_proj.weight\n",
      "model.language_model.layers.10.mlp.up_proj.weight\n",
      "model.language_model.layers.10.mlp.down_proj.weight\n",
      "model.language_model.layers.10.input_layernorm.weight\n",
      "model.language_model.layers.10.post_attention_layernorm.weight\n",
      "model.language_model.layers.11.self_attn.q_proj.weight\n",
      "model.language_model.layers.11.self_attn.k_proj.weight\n",
      "model.language_model.layers.11.self_attn.v_proj.weight\n",
      "model.language_model.layers.11.self_attn.o_proj.weight\n",
      "model.language_model.layers.11.self_attn.q_norm.weight\n",
      "model.language_model.layers.11.self_attn.k_norm.weight\n",
      "model.language_model.layers.11.mlp.gate_proj.weight\n",
      "model.language_model.layers.11.mlp.up_proj.weight\n",
      "model.language_model.layers.11.mlp.down_proj.weight\n",
      "model.language_model.layers.11.input_layernorm.weight\n",
      "model.language_model.layers.11.post_attention_layernorm.weight\n",
      "model.language_model.layers.12.self_attn.q_proj.weight\n",
      "model.language_model.layers.12.self_attn.k_proj.weight\n",
      "model.language_model.layers.12.self_attn.v_proj.weight\n",
      "model.language_model.layers.12.self_attn.o_proj.weight\n",
      "model.language_model.layers.12.self_attn.q_norm.weight\n",
      "model.language_model.layers.12.self_attn.k_norm.weight\n",
      "model.language_model.layers.12.mlp.gate_proj.weight\n",
      "model.language_model.layers.12.mlp.up_proj.weight\n",
      "model.language_model.layers.12.mlp.down_proj.weight\n",
      "model.language_model.layers.12.input_layernorm.weight\n",
      "model.language_model.layers.12.post_attention_layernorm.weight\n",
      "model.language_model.layers.13.self_attn.q_proj.weight\n",
      "model.language_model.layers.13.self_attn.k_proj.weight\n",
      "model.language_model.layers.13.self_attn.v_proj.weight\n",
      "model.language_model.layers.13.self_attn.o_proj.weight\n",
      "model.language_model.layers.13.self_attn.q_norm.weight\n",
      "model.language_model.layers.13.self_attn.k_norm.weight\n",
      "model.language_model.layers.13.mlp.gate_proj.weight\n",
      "model.language_model.layers.13.mlp.up_proj.weight\n",
      "model.language_model.layers.13.mlp.down_proj.weight\n",
      "model.language_model.layers.13.input_layernorm.weight\n",
      "model.language_model.layers.13.post_attention_layernorm.weight\n",
      "model.language_model.layers.14.self_attn.q_proj.weight\n",
      "model.language_model.layers.14.self_attn.k_proj.weight\n",
      "model.language_model.layers.14.self_attn.v_proj.weight\n",
      "model.language_model.layers.14.self_attn.o_proj.weight\n",
      "model.language_model.layers.14.self_attn.q_norm.weight\n",
      "model.language_model.layers.14.self_attn.k_norm.weight\n",
      "model.language_model.layers.14.mlp.gate_proj.weight\n",
      "model.language_model.layers.14.mlp.up_proj.weight\n",
      "model.language_model.layers.14.mlp.down_proj.weight\n",
      "model.language_model.layers.14.input_layernorm.weight\n",
      "model.language_model.layers.14.post_attention_layernorm.weight\n",
      "model.language_model.layers.15.self_attn.q_proj.weight\n",
      "model.language_model.layers.15.self_attn.k_proj.weight\n",
      "model.language_model.layers.15.self_attn.v_proj.weight\n",
      "model.language_model.layers.15.self_attn.o_proj.weight\n",
      "model.language_model.layers.15.self_attn.q_norm.weight\n",
      "model.language_model.layers.15.self_attn.k_norm.weight\n",
      "model.language_model.layers.15.mlp.gate_proj.weight\n",
      "model.language_model.layers.15.mlp.up_proj.weight\n",
      "model.language_model.layers.15.mlp.down_proj.weight\n",
      "model.language_model.layers.15.input_layernorm.weight\n",
      "model.language_model.layers.15.post_attention_layernorm.weight\n",
      "model.language_model.layers.16.self_attn.q_proj.weight\n",
      "model.language_model.layers.16.self_attn.k_proj.weight\n",
      "model.language_model.layers.16.self_attn.v_proj.weight\n",
      "model.language_model.layers.16.self_attn.o_proj.weight\n",
      "model.language_model.layers.16.self_attn.q_norm.weight\n",
      "model.language_model.layers.16.self_attn.k_norm.weight\n",
      "model.language_model.layers.16.mlp.gate_proj.weight\n",
      "model.language_model.layers.16.mlp.up_proj.weight\n",
      "model.language_model.layers.16.mlp.down_proj.weight\n",
      "model.language_model.layers.16.input_layernorm.weight\n",
      "model.language_model.layers.16.post_attention_layernorm.weight\n",
      "model.language_model.layers.17.self_attn.q_proj.weight\n",
      "model.language_model.layers.17.self_attn.k_proj.weight\n",
      "model.language_model.layers.17.self_attn.v_proj.weight\n",
      "model.language_model.layers.17.self_attn.o_proj.weight\n",
      "model.language_model.layers.17.self_attn.q_norm.weight\n",
      "model.language_model.layers.17.self_attn.k_norm.weight\n",
      "model.language_model.layers.17.mlp.gate_proj.weight\n",
      "model.language_model.layers.17.mlp.up_proj.weight\n",
      "model.language_model.layers.17.mlp.down_proj.weight\n",
      "model.language_model.layers.17.input_layernorm.weight\n",
      "model.language_model.layers.17.post_attention_layernorm.weight\n",
      "model.language_model.layers.18.self_attn.q_proj.weight\n",
      "model.language_model.layers.18.self_attn.k_proj.weight\n",
      "model.language_model.layers.18.self_attn.v_proj.weight\n",
      "model.language_model.layers.18.self_attn.o_proj.weight\n",
      "model.language_model.layers.18.self_attn.q_norm.weight\n",
      "model.language_model.layers.18.self_attn.k_norm.weight\n",
      "model.language_model.layers.18.mlp.gate_proj.weight\n",
      "model.language_model.layers.18.mlp.up_proj.weight\n",
      "model.language_model.layers.18.mlp.down_proj.weight\n",
      "model.language_model.layers.18.input_layernorm.weight\n",
      "model.language_model.layers.18.post_attention_layernorm.weight\n",
      "model.language_model.layers.19.self_attn.q_proj.weight\n",
      "model.language_model.layers.19.self_attn.k_proj.weight\n",
      "model.language_model.layers.19.self_attn.v_proj.weight\n",
      "model.language_model.layers.19.self_attn.o_proj.weight\n",
      "model.language_model.layers.19.self_attn.q_norm.weight\n",
      "model.language_model.layers.19.self_attn.k_norm.weight\n",
      "model.language_model.layers.19.mlp.gate_proj.weight\n",
      "model.language_model.layers.19.mlp.up_proj.weight\n",
      "model.language_model.layers.19.mlp.down_proj.weight\n",
      "model.language_model.layers.19.input_layernorm.weight\n",
      "model.language_model.layers.19.post_attention_layernorm.weight\n",
      "model.language_model.layers.20.self_attn.q_proj.weight\n",
      "model.language_model.layers.20.self_attn.k_proj.weight\n",
      "model.language_model.layers.20.self_attn.v_proj.weight\n",
      "model.language_model.layers.20.self_attn.o_proj.weight\n",
      "model.language_model.layers.20.self_attn.q_norm.weight\n",
      "model.language_model.layers.20.self_attn.k_norm.weight\n",
      "model.language_model.layers.20.mlp.gate_proj.weight\n",
      "model.language_model.layers.20.mlp.up_proj.weight\n",
      "model.language_model.layers.20.mlp.down_proj.weight\n",
      "model.language_model.layers.20.input_layernorm.weight\n",
      "model.language_model.layers.20.post_attention_layernorm.weight\n",
      "model.language_model.layers.21.self_attn.q_proj.weight\n",
      "model.language_model.layers.21.self_attn.k_proj.weight\n",
      "model.language_model.layers.21.self_attn.v_proj.weight\n",
      "model.language_model.layers.21.self_attn.o_proj.weight\n",
      "model.language_model.layers.21.self_attn.q_norm.weight\n",
      "model.language_model.layers.21.self_attn.k_norm.weight\n",
      "model.language_model.layers.21.mlp.gate_proj.weight\n",
      "model.language_model.layers.21.mlp.up_proj.weight\n",
      "model.language_model.layers.21.mlp.down_proj.weight\n",
      "model.language_model.layers.21.input_layernorm.weight\n",
      "model.language_model.layers.21.post_attention_layernorm.weight\n",
      "model.language_model.layers.22.self_attn.q_proj.weight\n",
      "model.language_model.layers.22.self_attn.k_proj.weight\n",
      "model.language_model.layers.22.self_attn.v_proj.weight\n",
      "model.language_model.layers.22.self_attn.o_proj.weight\n",
      "model.language_model.layers.22.self_attn.q_norm.weight\n",
      "model.language_model.layers.22.self_attn.k_norm.weight\n",
      "model.language_model.layers.22.mlp.gate_proj.weight\n",
      "model.language_model.layers.22.mlp.up_proj.weight\n",
      "model.language_model.layers.22.mlp.down_proj.weight\n",
      "model.language_model.layers.22.input_layernorm.weight\n",
      "model.language_model.layers.22.post_attention_layernorm.weight\n",
      "model.language_model.layers.23.self_attn.q_proj.weight\n",
      "model.language_model.layers.23.self_attn.k_proj.weight\n",
      "model.language_model.layers.23.self_attn.v_proj.weight\n",
      "model.language_model.layers.23.self_attn.o_proj.weight\n",
      "model.language_model.layers.23.self_attn.q_norm.weight\n",
      "model.language_model.layers.23.self_attn.k_norm.weight\n",
      "model.language_model.layers.23.mlp.gate_proj.weight\n",
      "model.language_model.layers.23.mlp.up_proj.weight\n",
      "model.language_model.layers.23.mlp.down_proj.weight\n",
      "model.language_model.layers.23.input_layernorm.weight\n",
      "model.language_model.layers.23.post_attention_layernorm.weight\n",
      "model.language_model.layers.24.self_attn.q_proj.weight\n",
      "model.language_model.layers.24.self_attn.k_proj.weight\n",
      "model.language_model.layers.24.self_attn.v_proj.weight\n",
      "model.language_model.layers.24.self_attn.o_proj.weight\n",
      "model.language_model.layers.24.self_attn.q_norm.weight\n",
      "model.language_model.layers.24.self_attn.k_norm.weight\n",
      "model.language_model.layers.24.mlp.gate_proj.weight\n",
      "model.language_model.layers.24.mlp.up_proj.weight\n",
      "model.language_model.layers.24.mlp.down_proj.weight\n",
      "model.language_model.layers.24.input_layernorm.weight\n",
      "model.language_model.layers.24.post_attention_layernorm.weight\n",
      "model.language_model.layers.25.self_attn.q_proj.weight\n",
      "model.language_model.layers.25.self_attn.k_proj.weight\n",
      "model.language_model.layers.25.self_attn.v_proj.weight\n",
      "model.language_model.layers.25.self_attn.o_proj.weight\n",
      "model.language_model.layers.25.self_attn.q_norm.weight\n",
      "model.language_model.layers.25.self_attn.k_norm.weight\n",
      "model.language_model.layers.25.mlp.gate_proj.weight\n",
      "model.language_model.layers.25.mlp.up_proj.weight\n",
      "model.language_model.layers.25.mlp.down_proj.weight\n",
      "model.language_model.layers.25.input_layernorm.weight\n",
      "model.language_model.layers.25.post_attention_layernorm.weight\n",
      "model.language_model.layers.26.self_attn.q_proj.weight\n",
      "model.language_model.layers.26.self_attn.k_proj.weight\n",
      "model.language_model.layers.26.self_attn.v_proj.weight\n",
      "model.language_model.layers.26.self_attn.o_proj.weight\n",
      "model.language_model.layers.26.self_attn.q_norm.weight\n",
      "model.language_model.layers.26.self_attn.k_norm.weight\n",
      "model.language_model.layers.26.mlp.gate_proj.weight\n",
      "model.language_model.layers.26.mlp.up_proj.weight\n",
      "model.language_model.layers.26.mlp.down_proj.weight\n",
      "model.language_model.layers.26.input_layernorm.weight\n",
      "model.language_model.layers.26.post_attention_layernorm.weight\n",
      "model.language_model.layers.27.self_attn.q_proj.weight\n",
      "model.language_model.layers.27.self_attn.k_proj.weight\n",
      "model.language_model.layers.27.self_attn.v_proj.weight\n",
      "model.language_model.layers.27.self_attn.o_proj.weight\n",
      "model.language_model.layers.27.self_attn.q_norm.weight\n",
      "model.language_model.layers.27.self_attn.k_norm.weight\n",
      "model.language_model.layers.27.mlp.gate_proj.weight\n",
      "model.language_model.layers.27.mlp.up_proj.weight\n",
      "model.language_model.layers.27.mlp.down_proj.weight\n",
      "model.language_model.layers.27.input_layernorm.weight\n",
      "model.language_model.layers.27.post_attention_layernorm.weight\n",
      "model.language_model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,716,288 || all params: 2,136,248,320 || trainable%: 0.4080\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trainable_params = 0\n",
    "for name,param in model.named_parameters():\n",
    "    if \"language_model.layers.27\" in name or \"language_model.norm\" in name:\n",
    "        param.requires_grad = True\n",
    "        num_trainable_params += param.numel()\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters (last layer and norm):50.34M parameters\n",
      "Trainable parameters ratio: 0.023660\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of trainable parameters (last layer and norm):{num_trainable_params / 1e6:.2f}M parameters\")\n",
    "print(f\"Trainable parameters ratio: {num_trainable_params / total_params:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PromptTuningConfig, get_peft_model,TaskType,PromptTuningInit\n",
    "\n",
    "# \n",
    "config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                            prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "                            prompt_tuning_init_text=\",\",\n",
    "                            num_virtual_tokens=20,\n",
    "                            tokenizer_name_or_path=\"/mnt/data1/ygm/models/Qwen3-VL-2B-Instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTuningConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, num_virtual_tokens=20, token_dim=None, num_transformer_submodules=None, num_attention_heads=None, num_layers=None, prompt_tuning_init=<PromptTuningInit.TEXT: 'TEXT'>, prompt_tuning_init_text=',', tokenizer_name_or_path='/mnt/data1/ygm/models/Qwen3-VL-2B-Instruct', tokenizer_kwargs=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type='FEATURE_EXTRACTION', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules='(.*(model)(?!.*visual).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$|.*(custom_text_proj).*$)', exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig,TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    bias=\"none\",\n",
    "    task_type=\"FEATURE_EXTRACTION\",\n",
    "    target_modules=\"(.*(model)(?!.*visual).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$|.*(custom_text_proj).*$)\",\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForFeatureExtraction(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3VLForConditionalGeneration(\n",
       "      (model): Qwen3VLModel(\n",
       "        (visual): Qwen3VLVisionModel(\n",
       "          (patch_embed): Qwen3VLVisionPatchEmbed(\n",
       "            (proj): Conv3d(3, 1024, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "          )\n",
       "          (pos_embed): Embedding(2304, 1024)\n",
       "          (rotary_pos_emb): Qwen3VLVisionRotaryEmbedding()\n",
       "          (blocks): ModuleList(\n",
       "            (0-23): 24 x Qwen3VLVisionBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Qwen3VLVisionAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (mlp): Qwen3VLVisionMLP(\n",
       "                (linear_fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (linear_fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (act_fn): GELUTanh()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (merger): Qwen3VLVisionPatchMerger(\n",
       "            (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (act_fn): GELU(approximate='none')\n",
       "            (linear_fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "          )\n",
       "          (deepstack_merger_list): ModuleList(\n",
       "            (0-2): 3 x Qwen3VLVisionPatchMerger(\n",
       "              (norm): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n",
       "              (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "              (act_fn): GELU(approximate='none')\n",
       "              (linear_fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (language_model): Qwen3VLTextModel(\n",
       "          (embed_tokens): Embedding(151936, 2048)\n",
       "          (layers): ModuleList(\n",
       "            (0-27): 28 x Qwen3VLTextDecoderLayer(\n",
       "              (self_attn): Qwen3VLTextAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
       "                (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Qwen3VLTextMLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=6144, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=6144, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=6144, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): SiLUActivation()\n",
       "              )\n",
       "              (input_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n",
       "              (post_attention_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n",
       "          (rotary_emb): Qwen3VLTextRotaryEmbedding()\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.visual.patch_embed.proj.weight\n",
      "base_model.model.model.visual.patch_embed.proj.bias\n",
      "base_model.model.model.visual.pos_embed.weight\n",
      "base_model.model.model.visual.blocks.0.norm1.weight\n",
      "base_model.model.model.visual.blocks.0.norm1.bias\n",
      "base_model.model.model.visual.blocks.0.norm2.weight\n",
      "base_model.model.model.visual.blocks.0.norm2.bias\n",
      "base_model.model.model.visual.blocks.0.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.0.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.0.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.0.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.0.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.0.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.0.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.0.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.1.norm1.weight\n",
      "base_model.model.model.visual.blocks.1.norm1.bias\n",
      "base_model.model.model.visual.blocks.1.norm2.weight\n",
      "base_model.model.model.visual.blocks.1.norm2.bias\n",
      "base_model.model.model.visual.blocks.1.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.1.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.1.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.1.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.1.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.1.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.1.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.1.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.2.norm1.weight\n",
      "base_model.model.model.visual.blocks.2.norm1.bias\n",
      "base_model.model.model.visual.blocks.2.norm2.weight\n",
      "base_model.model.model.visual.blocks.2.norm2.bias\n",
      "base_model.model.model.visual.blocks.2.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.2.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.2.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.2.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.2.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.2.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.2.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.2.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.3.norm1.weight\n",
      "base_model.model.model.visual.blocks.3.norm1.bias\n",
      "base_model.model.model.visual.blocks.3.norm2.weight\n",
      "base_model.model.model.visual.blocks.3.norm2.bias\n",
      "base_model.model.model.visual.blocks.3.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.3.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.3.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.3.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.3.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.3.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.3.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.3.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.4.norm1.weight\n",
      "base_model.model.model.visual.blocks.4.norm1.bias\n",
      "base_model.model.model.visual.blocks.4.norm2.weight\n",
      "base_model.model.model.visual.blocks.4.norm2.bias\n",
      "base_model.model.model.visual.blocks.4.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.4.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.4.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.4.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.4.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.4.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.4.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.4.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.5.norm1.weight\n",
      "base_model.model.model.visual.blocks.5.norm1.bias\n",
      "base_model.model.model.visual.blocks.5.norm2.weight\n",
      "base_model.model.model.visual.blocks.5.norm2.bias\n",
      "base_model.model.model.visual.blocks.5.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.5.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.5.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.5.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.5.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.5.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.5.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.5.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.6.norm1.weight\n",
      "base_model.model.model.visual.blocks.6.norm1.bias\n",
      "base_model.model.model.visual.blocks.6.norm2.weight\n",
      "base_model.model.model.visual.blocks.6.norm2.bias\n",
      "base_model.model.model.visual.blocks.6.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.6.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.6.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.6.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.6.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.6.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.6.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.6.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.7.norm1.weight\n",
      "base_model.model.model.visual.blocks.7.norm1.bias\n",
      "base_model.model.model.visual.blocks.7.norm2.weight\n",
      "base_model.model.model.visual.blocks.7.norm2.bias\n",
      "base_model.model.model.visual.blocks.7.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.7.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.7.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.7.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.7.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.7.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.7.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.7.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.8.norm1.weight\n",
      "base_model.model.model.visual.blocks.8.norm1.bias\n",
      "base_model.model.model.visual.blocks.8.norm2.weight\n",
      "base_model.model.model.visual.blocks.8.norm2.bias\n",
      "base_model.model.model.visual.blocks.8.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.8.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.8.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.8.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.8.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.8.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.8.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.8.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.9.norm1.weight\n",
      "base_model.model.model.visual.blocks.9.norm1.bias\n",
      "base_model.model.model.visual.blocks.9.norm2.weight\n",
      "base_model.model.model.visual.blocks.9.norm2.bias\n",
      "base_model.model.model.visual.blocks.9.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.9.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.9.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.9.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.9.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.9.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.9.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.9.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.10.norm1.weight\n",
      "base_model.model.model.visual.blocks.10.norm1.bias\n",
      "base_model.model.model.visual.blocks.10.norm2.weight\n",
      "base_model.model.model.visual.blocks.10.norm2.bias\n",
      "base_model.model.model.visual.blocks.10.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.10.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.10.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.10.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.10.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.10.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.10.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.10.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.11.norm1.weight\n",
      "base_model.model.model.visual.blocks.11.norm1.bias\n",
      "base_model.model.model.visual.blocks.11.norm2.weight\n",
      "base_model.model.model.visual.blocks.11.norm2.bias\n",
      "base_model.model.model.visual.blocks.11.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.11.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.11.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.11.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.11.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.11.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.11.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.11.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.12.norm1.weight\n",
      "base_model.model.model.visual.blocks.12.norm1.bias\n",
      "base_model.model.model.visual.blocks.12.norm2.weight\n",
      "base_model.model.model.visual.blocks.12.norm2.bias\n",
      "base_model.model.model.visual.blocks.12.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.12.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.12.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.12.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.12.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.12.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.12.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.12.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.13.norm1.weight\n",
      "base_model.model.model.visual.blocks.13.norm1.bias\n",
      "base_model.model.model.visual.blocks.13.norm2.weight\n",
      "base_model.model.model.visual.blocks.13.norm2.bias\n",
      "base_model.model.model.visual.blocks.13.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.13.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.13.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.13.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.13.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.13.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.13.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.13.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.14.norm1.weight\n",
      "base_model.model.model.visual.blocks.14.norm1.bias\n",
      "base_model.model.model.visual.blocks.14.norm2.weight\n",
      "base_model.model.model.visual.blocks.14.norm2.bias\n",
      "base_model.model.model.visual.blocks.14.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.14.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.14.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.14.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.14.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.14.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.14.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.14.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.15.norm1.weight\n",
      "base_model.model.model.visual.blocks.15.norm1.bias\n",
      "base_model.model.model.visual.blocks.15.norm2.weight\n",
      "base_model.model.model.visual.blocks.15.norm2.bias\n",
      "base_model.model.model.visual.blocks.15.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.15.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.15.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.15.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.15.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.15.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.15.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.15.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.16.norm1.weight\n",
      "base_model.model.model.visual.blocks.16.norm1.bias\n",
      "base_model.model.model.visual.blocks.16.norm2.weight\n",
      "base_model.model.model.visual.blocks.16.norm2.bias\n",
      "base_model.model.model.visual.blocks.16.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.16.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.16.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.16.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.16.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.16.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.16.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.16.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.17.norm1.weight\n",
      "base_model.model.model.visual.blocks.17.norm1.bias\n",
      "base_model.model.model.visual.blocks.17.norm2.weight\n",
      "base_model.model.model.visual.blocks.17.norm2.bias\n",
      "base_model.model.model.visual.blocks.17.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.17.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.17.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.17.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.17.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.17.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.17.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.17.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.18.norm1.weight\n",
      "base_model.model.model.visual.blocks.18.norm1.bias\n",
      "base_model.model.model.visual.blocks.18.norm2.weight\n",
      "base_model.model.model.visual.blocks.18.norm2.bias\n",
      "base_model.model.model.visual.blocks.18.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.18.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.18.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.18.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.18.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.18.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.18.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.18.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.19.norm1.weight\n",
      "base_model.model.model.visual.blocks.19.norm1.bias\n",
      "base_model.model.model.visual.blocks.19.norm2.weight\n",
      "base_model.model.model.visual.blocks.19.norm2.bias\n",
      "base_model.model.model.visual.blocks.19.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.19.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.19.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.19.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.19.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.19.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.19.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.19.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.20.norm1.weight\n",
      "base_model.model.model.visual.blocks.20.norm1.bias\n",
      "base_model.model.model.visual.blocks.20.norm2.weight\n",
      "base_model.model.model.visual.blocks.20.norm2.bias\n",
      "base_model.model.model.visual.blocks.20.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.20.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.20.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.20.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.20.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.20.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.20.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.20.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.21.norm1.weight\n",
      "base_model.model.model.visual.blocks.21.norm1.bias\n",
      "base_model.model.model.visual.blocks.21.norm2.weight\n",
      "base_model.model.model.visual.blocks.21.norm2.bias\n",
      "base_model.model.model.visual.blocks.21.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.21.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.21.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.21.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.21.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.21.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.21.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.21.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.22.norm1.weight\n",
      "base_model.model.model.visual.blocks.22.norm1.bias\n",
      "base_model.model.model.visual.blocks.22.norm2.weight\n",
      "base_model.model.model.visual.blocks.22.norm2.bias\n",
      "base_model.model.model.visual.blocks.22.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.22.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.22.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.22.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.22.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.22.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.22.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.22.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.blocks.23.norm1.weight\n",
      "base_model.model.model.visual.blocks.23.norm1.bias\n",
      "base_model.model.model.visual.blocks.23.norm2.weight\n",
      "base_model.model.model.visual.blocks.23.norm2.bias\n",
      "base_model.model.model.visual.blocks.23.attn.qkv.weight\n",
      "base_model.model.model.visual.blocks.23.attn.qkv.bias\n",
      "base_model.model.model.visual.blocks.23.attn.proj.weight\n",
      "base_model.model.model.visual.blocks.23.attn.proj.bias\n",
      "base_model.model.model.visual.blocks.23.mlp.linear_fc1.weight\n",
      "base_model.model.model.visual.blocks.23.mlp.linear_fc1.bias\n",
      "base_model.model.model.visual.blocks.23.mlp.linear_fc2.weight\n",
      "base_model.model.model.visual.blocks.23.mlp.linear_fc2.bias\n",
      "base_model.model.model.visual.merger.norm.weight\n",
      "base_model.model.model.visual.merger.norm.bias\n",
      "base_model.model.model.visual.merger.linear_fc1.weight\n",
      "base_model.model.model.visual.merger.linear_fc1.bias\n",
      "base_model.model.model.visual.merger.linear_fc2.weight\n",
      "base_model.model.model.visual.merger.linear_fc2.bias\n",
      "base_model.model.model.visual.deepstack_merger_list.0.norm.weight\n",
      "base_model.model.model.visual.deepstack_merger_list.0.norm.bias\n",
      "base_model.model.model.visual.deepstack_merger_list.0.linear_fc1.weight\n",
      "base_model.model.model.visual.deepstack_merger_list.0.linear_fc1.bias\n",
      "base_model.model.model.visual.deepstack_merger_list.0.linear_fc2.weight\n",
      "base_model.model.model.visual.deepstack_merger_list.0.linear_fc2.bias\n",
      "base_model.model.model.visual.deepstack_merger_list.1.norm.weight\n",
      "base_model.model.model.visual.deepstack_merger_list.1.norm.bias\n",
      "base_model.model.model.visual.deepstack_merger_list.1.linear_fc1.weight\n",
      "base_model.model.model.visual.deepstack_merger_list.1.linear_fc1.bias\n",
      "base_model.model.model.visual.deepstack_merger_list.1.linear_fc2.weight\n",
      "base_model.model.model.visual.deepstack_merger_list.1.linear_fc2.bias\n",
      "base_model.model.model.visual.deepstack_merger_list.2.norm.weight\n",
      "base_model.model.model.visual.deepstack_merger_list.2.norm.bias\n",
      "base_model.model.model.visual.deepstack_merger_list.2.linear_fc1.weight\n",
      "base_model.model.model.visual.deepstack_merger_list.2.linear_fc1.bias\n",
      "base_model.model.model.visual.deepstack_merger_list.2.linear_fc2.weight\n",
      "base_model.model.model.visual.deepstack_merger_list.2.linear_fc2.bias\n",
      "base_model.model.model.language_model.embed_tokens.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.0.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.0.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.0.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.1.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.1.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.1.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.2.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.2.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.2.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.3.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.3.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.3.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.4.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.4.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.4.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.5.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.5.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.5.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.6.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.6.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.6.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.7.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.7.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.7.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.8.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.8.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.8.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.9.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.9.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.9.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.10.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.10.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.10.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.11.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.11.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.11.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.12.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.12.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.12.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.13.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.13.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.13.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.14.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.14.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.14.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.15.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.15.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.15.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.16.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.16.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.16.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.17.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.17.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.17.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.18.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.18.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.18.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.19.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.19.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.19.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.20.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.20.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.20.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.21.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.21.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.21.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.22.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.22.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.22.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.23.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.23.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.23.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.24.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.24.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.24.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.25.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.25.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.25.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.26.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.26.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.26.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.q_norm.weight\n",
      "base_model.model.model.language_model.layers.27.self_attn.k_norm.weight\n",
      "base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight\n",
      "base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight\n",
      "base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight\n",
      "base_model.model.model.language_model.layers.27.input_layernorm.weight\n",
      "base_model.model.model.language_model.layers.27.post_attention_layernorm.weight\n",
      "base_model.model.model.language_model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,716,288 || all params: 2,136,248,320 || trainable%: 0.4080\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colpali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
