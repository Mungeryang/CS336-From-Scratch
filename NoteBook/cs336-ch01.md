<center>
  <h1>CS336: Language Modeling from Scratch</h1>
</center>

<center>
  <h1>Stanford / Spring 2025</h1>
</center>

<center>
  å­¦ä¹ ç¬”è®°æ•´ç†ï¼šæ¨æ¡‚æ·¼
</center>
<center>
  è¯¾ç¨‹ä¸»é¡µï¼šhttps://stanford-cs336.github.io/spring2025/
</center>

## Goal

This course is designed to provide students with a comprehensive understanding of language models by walking them through the entire process of developing their own. Drawing inspiration from operating systems courses that create an entire operating system from scratch, we will lead students through every aspect of language model creation, including data collection and cleaning for pre-training, transformer model construction, model training, and evaluation before deployment.

![design-decisions](https://stanford-cs336.github.io/spring2025-lectures/images/design-decisions.png)

## Tokenization

Tokenizersæ˜¯å°†å­—ç¬¦ä¸²å’Œæ•´æ•°åºåˆ—ä¹‹é—´è¿›è¡Œç›¸äº’è½¬æ¢ï¼Œå››ç±»ä¸»è¦çš„åˆ†è¯å™¨ï¼šcharacter_tokenizerã€byte_tokenizerã€word_tokenizerã€bpe_tokenizerã€‚

![Tokenization](https://stanford-cs336.github.io/spring2025-lectures/images/tokenized-example.png)

character_tokenizerï¼šæ¯ä¸ªå­—ç¬¦å¯ä»¥é€šè¿‡`ord`å‡½æ•°å®Œæˆå­—ç¬¦åˆ°ASCIIç (Unicode)çš„ç›¸äº’è½¬æ¢ã€‚ä½†æ˜¯å¸¦æ¥çš„é—®é¢˜å°±æ˜¯æ„æˆçš„è¯è¡¨éå¸¸å¤§ï¼Œè®¸å¤šå­—ç¬¦ç”Ÿåƒ»å°‘ç”¨å¯¼è‡´è¯æ±‡ä½¿ç”¨æ•ˆç‡ä½ã€‚

byte_tokenizerï¼šUnicodeå­—ç¬¦å¯ä»¥è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªä½¿ç”¨0-255æ•´æ•°è¡¨ç¤ºçš„**å­—èŠ‚åºåˆ—**ã€‚ä½†æ˜¯ç”±äºä¸€ä¸ªå­—èŠ‚åªèƒ½è¢«256ä¸ªæ•´æ•°æ‰€è¡¨ç¤ºï¼Œè¿™å°±é€ æˆäº†ç»è¿‡Tokenizationåçš„åºåˆ—é•¿åº¦ä¼šå˜å¾—éå¸¸é•¿ã€‚

word_tokenizerï¼šNLPé¢†åŸŸå†…å¸¸ç”¨çš„æ–¹æ³•æ˜¯å°†å­—ç¬¦ä¸²åˆ†å‰²ä¸ºå•è¯ï¼Œåˆ©ç”¨æ­£åˆ™è¡¨è¾¾å¼æ„å»ºåˆ†è¯å™¨ï¼Œå°†åŸºäºåˆ†è¯å™¨å¾—åˆ°çš„segmentsæ˜ å°„ä¸ºæ•´æ•°ï¼Œæ„å»ºå‡ºæ¯ä¸ªsegmentçš„æ•´æ•°æ˜ å°„ã€‚ä½†æ˜¯å­˜åœ¨çš„é—®é¢˜æ˜¯å•è¯çš„æ•°é‡å·¨å¤§ï¼Œæ— æ³•æä¾›å›ºå®šçš„è¯æ±‡é‡ï¼›è®­ç»ƒæœŸé—´æœªä½¿ç”¨è¿‡çš„æ–°è¯ä¼šè¢«æ ‡è®°ä¸ºUNKï¼Œæ‰°ä¹±å›°æƒ‘åº¦è®¡ç®—ã€‚

ğŸ’¡è”æƒ³è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼šLLMçš„è¾“å‡ºæœ¬è´¨ä¸Šå°±æ˜¯åœ¨ä¸€ä¸ªå¤§çš„è¯è¡¨ä¸Šé¢„æµ‹å•è¯çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¦‚æœè¯è¡¨éå¸¸å¤§ï¼Œé‚£ä¹ˆè®¡ç®—äº†å¯æƒ³è€ŒçŸ¥æ˜¯éå¸¸ææ€–çš„ï¼Œè€Œä¸”ä¹Ÿä¼šå½±å“tokençš„é¢„æµ‹ã€‚ä¸Šé¢ä»‹ç»çš„ä¸‰ç§Tokenizationéƒ½å„æœ‰ç‰¹ç‚¹ï¼Œä½†æ˜¯ä¹Ÿéƒ½å­˜åœ¨è‡´å‘½çš„ç¼ºç‚¹ã€‚LLMåŸºäºä¸Šè¿°ç¼–ç çš„åŠ£åŠ¿ï¼Œæœ€ç»ˆé€‰æ‹©é‡‡ç”¨BPEç®—æ³•ä½œä¸ºæ¨¡å‹çš„Tokenizationæ–¹æ³•ã€‚

### Byte Pair Encoding

BPEç®—æ³•æ˜¯åœ¨1994å¹´å‘è¡¨çš„â€œA New Algorithm for Data Compressionâ€æå‡ºçš„ä¸€ç§æ–°å‹**æ•°æ®å‹ç¼©**ç®—æ³•ã€‚æ ¸å¿ƒæ€æƒ³å°±æ˜¯å°†åºåˆ—ä¸­å¸¸è§çš„ä¸€å¯¹**ç›¸é‚»çš„æ•°æ®å•å…ƒ**æ›¿æ¢ä¸ºæ•°æ®ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„æ–°å•å…ƒï¼Œåå¤è¿­ä»£ç›´è‡³æ»¡è¶³ç»ˆæ­¢æ¡ä»¶ã€‚

bpe_tokenizerï¼šåŸºæœ¬æ€è·¯æ˜¯å¸¸è§çš„å­—ç¬¦åºåˆ—è¢«å•ä¸ªtokenè¡¨ç¤ºï¼Œç½•è§çš„åºåˆ—è¢«å¤šä¸ªtokensè¡¨ç¤ºã€‚

BPEçš„åŸºæœ¬è®¡ç®—è·¯ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š

<img src="/img/fig/bpe.jpg"/>





##  Pytorch and Resource accounting

### tensors_memory

Higher precision: more accurate/stable, more memory, more compute

Lower precision: less accurate/stable, less memory, less compute

- float32ï¼šå•ç²¾åº¦æµ®ç‚¹æ•°æ˜¯tensorçš„é»˜è®¤æ•°æ®ç±»å‹ï¼Œ4ä¸ªå­—èŠ‚è¡¨ç¤ºä¸€ä¸ªæµ®ç‚¹æ•°ã€‚1ä½ç¬¦å·ä½ï¼Œ 8ä½æŒ‡æ•°ä½ï¼Œ 23ä½å°æ•°ä½ã€‚ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶ï¼Œåˆ›å»ºtensoré»˜è®¤çš„æ•°æ®ç±»å‹ã€‚

å½“ç¬¦å·ä½ä¸º0ï¼ŒæŒ‡æ•°ä½ä¸º11111110ï¼Œå°¾æ•°ä½ä¸ºå…¨1æ—¶ï¼Œfloat32çš„æœ€å¤§å€¼ä¸º $2^{127} \times (1+\frac{2^{23}-1}{2^{23}}=3.40e38$ï¼›

å½“ç¬¦å·ä½ä¸º1ï¼ŒæŒ‡æ•°ä½ä¸º11111110ï¼Œå°¾æ•°ä½ä¸ºå…¨1æ—¶ï¼Œfloat32çš„æœ€å°å€¼ä¸º$-2^{127} \times (1+\frac{2^{23}-1}{2^{23}}=-3.40e38$ã€‚

- float16ï¼šåŠç²¾åº¦æµ®ç‚¹æ•°ï¼Œä¸¤ä¸ªå­—èŠ‚è¡¨ç¤ºä¸€ä¸ªæµ®ç‚¹æ•°ã€‚1ä½ç¬¦å·ä½ï¼Œ 5ä½æŒ‡æ•°ä½ï¼Œ 10ä½å°æ•°ä½ã€‚ç›¸æ¯”äºfloat32æ‰€å å†…å­˜æ›´å°ï¼Œä½†æ˜¯ä¼šå‡ºç°**æº¢å‡ºé—®é¢˜**ã€‚

å½“ç¬¦å·ä½ä¸º0ï¼ŒæŒ‡æ•°ä½ä¸º11110ï¼Œå°¾æ•°ä½ä¸ºå…¨1æ—¶ï¼Œfloat16çš„æœ€å¤§å€¼ä¸º $2^{15} \times (1+\frac{1023}{1024})=65504$ï¼›

å½“ç¬¦å·ä½ä¸º1ï¼ŒæŒ‡æ•°ä½ä¸º11110ï¼Œå°¾æ•°ä½ä¸ºå…¨1æ—¶ï¼Œfloat16çš„æœ€å°å€¼ä¸º$-2^{15} \times (1+\frac{1023}{1024}) = -65504$ã€‚

- bfloat16ï¼šä¸float16æ‰€å å†…å­˜ä¸€è‡´ï¼Œä½†æ˜¯å…·æœ‰float32çš„å­˜å‚¨èŒƒå›´ã€‚1ä½ç¬¦å·ä½ï¼Œ 8ä½æŒ‡æ•°ä½ï¼Œ 7ä½å°æ•°ä½ã€‚

è®¡ç®—`çŸ©é˜µä¹˜æ³•`ä¸`å‰å‘ä¼ æ’­`æ—¶æ¨èä½¿ç”¨bfloat16æ•°æ®ç±»å‹ï¼Œè®¡ç®—`Attentionæ³¨æ„åŠ›`æ—¶æ¨èä½¿ç”¨float32æ•°æ®ç±»å‹ã€‚

æŒ‡æ•°çš„åç½®ï¼š

- `float32` çš„ Bias ä¸º 127
- `float16` çš„ Bias ä¸º 15
- `bfloat16` çš„ Bias ä¸º 127

![FLOPS](https://githubraw.cdn.bcebos.com/PaddlePaddle/docs/develop/docs/guides/performance_improving/images/float.png?raw=true)

è®¡ç®—å®ä¾‹å¦‚ä¸‹ï¼Œä»¥float16ä¸ºä¾‹ï¼š

<img src="/img/fig/float16.jpg"/>

### tensors_on_gpus

ä¸ºäº†åˆ©ç”¨å¥½GPUsçš„å¹¶è¡Œä¼˜åŠ¿ï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®ä»CPUä¸­çš„RAMç§»åŠ¨åˆ°GPUä¸­çš„DRAMä¸­ï¼š

```python
if not torch.cuda.is_available():
  return

for i in range(num_gpus):
  properties = torch.cuda.get_device_properties(i)  # @inspect properties
 
y = x.to("cuda:0")
assert x.device == torch.device("cuda",0)
```

PyTorchä¸­çš„tensorså°±æ˜¯å­˜å‚¨åœ¨å†…å­˜ä¸­çš„æŒ‡é’ˆï¼Œå­˜å‚¨å½¢å¼å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![tensor-pointer](https://stanford-cs336.github.io/spring2025-lectures/var/files/image-97aa05a6701b46521cb8a7c1e096c7e7-https_martinlwx_github_io_img_2D_tensor_strides_png)

strides[1]è¡¨ç¤ºåˆ—æŒ‡é’ˆï¼Œstrides[0]è¡¨ç¤ºè¡ŒæŒ‡é’ˆï¼›ä¸Šå›¾ä¸­äºŒç»´æ ¼å¼ä¸ºæˆ‘ä»¬ä¹ æƒ¯ä¸Šç”»å‡ºçš„å­˜å‚¨æ ¼å¼ï¼Œä½†æ˜¯åœ¨å†…å­˜æ¡ä¸­å­˜å‚¨çš„æ ¼å¼æœ¬è´¨ä¸Šæ˜¯ä¸‹å›¾ä¸­çš„çº¿æ€§æ ¼å¼ã€‚å¦‚æœè¦å®šä½ä¸€ä¸ªå…ƒç´ çš„ä½ç½®(æŸ¥æ‰¾ä¸€ä¸ªå…ƒç´ )ï¼Œéœ€è¦åˆ©ç”¨è¿ä¸ªæŒ‡é’ˆè¿›è¡Œè¾…åŠ©ï¼š$pos=1 * strides[1] + 4 * strides[0]$ã€‚

ä»¥å…ƒç´ 10ä¸ºä¾‹(2è¡Œ3åˆ—)ï¼Œ$pos_{10}=1 * 3 + 4 * 2 = 11$ï¼Œé‚£ä¹ˆå†…å­˜ä¸­æ¨ªå‘ç¬¬11ä¸ªä½ç½®(ä»1è®¡æ•°)å¯¹åº”çš„å…ƒç´ å³æ˜¯10ã€‚

### tensor_operations

Pytorchå¯¹äºtensorçš„æ“ä½œä¸­ï¼Œ`view`æ˜¯freeï¼Œæ— é¡»å ç”¨å†…å­˜ï¼›ä½†æ˜¯copyæ“ä½œ(`contiguous and reshape`)æ˜¯éœ€è¦å†…å­˜å¼€é”€çš„ã€‚

âš ï¸**einops**æ˜¯ä¹‹å‰æ²¡æœ‰æ¥è§¦è¿‡çš„æ“ä½œï¼Œä¸»è¦ä½œç”¨æ˜¯é«˜æ•ˆæ“ä½œå¼ é‡çš„ç»´åº¦ï¼Œéœ€è¦ç•™å¿ƒå­¦ä¹ å…³æ³¨ä¸‹ã€‚æ¨èä¸€ç¯‡åšæ–‡ï¼šhttps://zhuanlan.zhihu.com/p/342675997

```python
# çŸ©é˜µä¹˜æ³•
def tensor_matmul():
  x = torch.ones(16, 32)
  w = torch.ones(32, 2)
  
y = x @ w
assert y.size() == torch.Size([16, 2])

# å®šä¹‰ä¸¤ä¸ªå¼ é‡:
x: Float[torch.Tensor, "batch seq1 hidden"] = torch.ones(2, 3, 4)  # @inspect x
y: Float[torch.Tensor, "batch seq2 hidden"] = torch.ones(2, 3, 4)  # @inspect y
def tensor_einops():
  #einopsä¸»è¦æ˜¯rearrange, reduce, repeatè¿™3ä¸ªæ–¹æ³•
  z = x @ y.transpose(-2, -1)  # batch, sequence, sequence  @inspect z --> Old way
  z = einsum(x, y, "batch seq1 hidden, batch seq2 hidden -> batch seq1 seq2")  # @inspect z --> New (einops) way
  z = einsum(x, y, "... seq1 hidden, ... seq2 hidden -> ... seq1 seq2")  # @inspect z
  # reduce
  x: Float[torch.Tensor, "batch seq hidden"] = torch.ones(2, 3, 4)  # @inspect x
  y = x.mean(dim=-1)  # @inspect y --> Old way
  y = reduce(x,"...hidden -> ...","sum") --> New (einops) way
  # rearrange
  x: Float[torch.Tensor, "batch seq total_hidden"] = torch.ones(2, 3, 8)  # @inspect x
  w: Float[torch.Tensor, "hidden1 hidden2"] = torch.ones(4,4)
  
  x = rearrange(x, "... (heads hidden1) -> ... heads hidden1", heads=2)  # @inspect x
  x = einsum(x, w, "... hidden1, hidden1 hidden2 -> ... hidden2")  # @inspect x
```

### Computational cost

LLMé‡Œçš„è®¡ç®—åŸºæœ¬ä¸Šéƒ½æ˜¯æµ®ç‚¹è¿ç®—(FLOP)ï¼Œç±»ä¼¼äºåŠ æ³•è¿ç®—å’Œä¹˜æ³•è¿ç®—ã€‚

å®¹æ˜“æ··æ·†çš„æ¦‚å¿µï¼š

- FLOPs: æµ®ç‚¹è¿ç®—æ¬¡æ•°
- FLOP/sï¼šæ¯ç§’é’Ÿæµ®ç‚¹è®¡ç®—æ¬¡æ•°ï¼Œè¡¡é‡ç¡¬ä»¶æŒ‡æ ‡

å¯¹ m x n çŸ©é˜µè¿›è¡Œå…ƒç´ è¿ç®—éœ€è¦ O(mn) FLOPsï¼›ä¸¤ä¸ª m x n çŸ©é˜µçš„åŠ æ³•éœ€è¦ mn æ¬¡ FLOPsã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ²¡æœ‰ä»»ä½•è¿ç®—æ¯”çŸ©é˜µä¹˜æ³•æ›´è€—æ—¶ã€‚

æˆ‘ä»¬ç”¨ B è¡¨ç¤ºæ•°æ®ç‚¹çš„ä¸ªæ•°ï¼Œ(D K)æ˜¯å‚æ•°å¤§å°ï¼Œ2 (# tokens) (# parameters)ä»£è¡¨å‰æƒ³ä¼ æ’­çš„FLOPsã€4 (# data points) (# parameters) åå‘ä¼ æ’­FLOPsã€6 (# data points) (# parameters)ä»£è¡¨ä¸€æ¬¡è®­ç»ƒçš„FLOPsã€‚

ä¸€ä¸ªé‡è¦æŒ‡æ ‡ï¼šModel FLOPs utilization - æ¨¡å‹FLOPsåˆ©ç”¨ç‡ï¼Œè®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

```python
mfu = actual_flop_per_sec / promised_flop_per_sec  # @inspect mfu
```

FLOP/så–å†³äºç¡¬ä»¶(H100 >> A100) å’Œæ•°æ®ç±»å‹(bfloat16 >> float32)ã€‚

### Parameter initialization

åˆå§‹åŒ–è¯­æ³•ï¼š

```python
x = nn.Parameter(torch.randn(input_dim))
w = nn.Parameter(nn.init.trunc_normal_(torch.empty(input_dim, output_dim), std=1 / np.sqrt(input_dim), a=-3, b=3))
```

ç»´åº¦ç¼©æ”¾çš„åŸå› ï¼šä¸ºäº†é˜²æ­¢å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­å‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼›å°†è¾“å…¥è¿›è¡Œ1/sqrt(input_dim) ç¼©æ”¾ï¼Œè¿™æ ·æ¯ä¸€æ­¥éƒ½ä¼šå¾—åˆ°åŸºäºæ­£å¤ªåˆ†å¸ƒçš„è¾“å‡ºï¼Œä¿è¯äº†æ•°æ®çš„ç¨³å®šæ€§ã€‚è¿™ä¸ªé“ç†åœ¨æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—ä¸­åŒæ ·é€‚ç”¨ã€‚

âš ï¸æ•°æ®åŠ è½½çš„æ³¨æ„äº‹é¡¹ï¼šDon't want to load the entire data into memory at onceï¼å¤§è¯­è¨€æ¨¡å‹çš„è¾“å…¥æ˜¯æ•´æ•°åºåˆ—ï¼Œå¯ä»¥ä½¿ç”¨numpyæ•°ç»„åŠ è½½è¿™äº›æ•°æ®ï¼Œä½¿ç”¨memapå»¶è¿ŸåŠ è½½éƒ¨åˆ†æ•°æ®åˆ°å†…å­˜ä¸­ã€‚

optimizerçš„ç»„æˆï¼š

- momentum = SGD + exponential averaging of grad
- AdaGrad = SGD + averaging by grad$^2$
- RMSProp = AdaGrad + exponentially averaging of grad$^2$
- Adam = RMSProp + momentum



